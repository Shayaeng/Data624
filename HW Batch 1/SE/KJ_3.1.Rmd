---
title: "KJ 3.1"
author: "Group 3"
date: "2024-06-05"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=F}
library(mlbench)
library(tidyverse)
library(ggcorrplot)
```

# Kuhn Johnson Chapter 3 homework 3.1

## Introduction

The UC Irvine Machine Learning Repository6 contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

The data can be accessed via:
```{r data}
data(Glass)
str(Glass)
```

## Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

### Density Plots

Using density plots reveals a lot about the distribution of the data. Some of the predictors appear to be pretty normally distributed, albeit with slight skew, while others don't.

- Al appears to be very normally distributed with a very slight right skew.
- Ba has a very large percentage of zeros. This can cause major issues for many models and skews the distribution to the right.
- Ca has a slight right skew.
- Fe, like Ba, has a very large percentage of zeros. 
- K has a significant amount of zeros while and most of the data is concentrated around 0.1 with what appear to be some major outliers around 0.6.
- Mg has what appears to be a normal distribution with a significant left skew. It also has a significant amount of zeros resulting in the appearance of a bimodal distribution. This is likely an illusion.
- Na, like Al, appears to be very normally distributed with a very slight right skew.
- Si appears to be very normally distributed with a slight left skew.
- RI (the refractive index) appears to be very normally distributed with a very slight right skew.

```{r density-plot}
Glass |>
  select(-Type) |>
  gather(key = "variable", value = "value") |>  
  ggplot(aes(x = value)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = '#4E79A7', color = 'black') + 
  stat_density(geom = "line", color = "red") +
  facet_wrap(~ variable, scales = 'free') +
  theme(strip.text = element_text(size = 5)) +
  theme_bw()
```

### Correlation Matrix

A correlation matrix can be used to understand the relationships between the predictors. The below plot reveals some significant relationships between the predictors. Most notably, there is a positive relationship of 0.81 between Ca and RI, this is an extremely strong relationship and should be considered when utilizing the data. There is also a significant positive relationship between Al and Ba (0.48), and negative relationships between RI and Si (-0.54), Mg and Ba (-0.49), and Mg and Al (-0.48) among some lesser but still potentially significant relationships. All these relationships should be considered when building a model and for any inference that may be drawn from the data. 

As noted above, some of the predictors have interesting distributions that might be improved with transformations. After those transformations, the relationships between the predictors may, and in some cases is likely to, change.

```{r corr-plot}
q <- cor(Glass |>
  select(-Type))

ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#F28E2B", "white", "#4E79A7"),
           lab = TRUE, show.legend = F, tl.cex = 10, lab_size = 3) 
```

## Do there appear to be any outliers in the data? Are any predictors skewed?

### Boxplots

As noted above by the density plots, there are definitely outliers in the data and some predictors are skewed. We can get additional information about the distribution of the data by using boxplots. 

The below boxplots confirm the presence of outliers in the data. The most notable outliers are in variables K and Ca, with Ba also having some significant outliers. The boxplots also confirm the presence of skew in the data. The most notable skew is in variables Ba and Fe, which have a significant amount of zeros.

```{r boxplot}
Glass |>
  select(-Type) |>
  gather(key = "variable", value = "value") |>
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot(fill = '#4E79A7', color = 'black') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_bw()
```

## Are there any relevant transformations of one or more predictors that might improve the classification model?

There definitely should be some transformations of the data. As noted above, some of the predictors have significant skew and can be improved with transformations. A box-cox transformation to find the optimal lambda for each predictor would be a good place to start. However, due to the large number of zeros in some of the variables, a constant should be added to the data before applying the transformation to avoid undefined values from the transformation. 

Additionally, the relationships between the predictors should be considered when transforming the data. For example, the strong relationship between Ca and RI should be considered when transforming the data and it might be decided to either use one or the other or to use the same transformation for both. It also might be decided to not transform any variables with relatively small skewness in order to not lose the relationships between predictors.

Another transformation that might be made is to scale and center the data (but not around zero if using a box-cox transformation as mentioned above) to ensure that all predictors are on the same scale. 